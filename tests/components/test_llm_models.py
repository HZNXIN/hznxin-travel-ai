"""
æµ‹è¯•ä¸åŒçš„æ¨¡å‹ï¼Œæ‰¾å‡ºå¯ç”¨çš„
"""

import sys
import os
sys.path.insert(0, os.path.dirname(__file__))

from src.core.llm_client import create_llm_client


API_KEY = "sk-UZp1D3qeRaYeDhDUVX55tHXjZfVW8qJHCJtgwmFJHuFAWKKN"
API_BASE = "https://hone.vvvv.ee/v1"

# å°è¯•ä¸åŒçš„æ¨¡å‹åç§°
models_to_try = [
    "gpt-3.5-turbo",
    "gpt-4",
    "gpt-4-turbo",
    "gpt-4o",
    "claude-3-opus",
    "claude-3-sonnet",
    "claude-3-haiku",
    "claude-3.5-sonnet",
    "deepseek-chat",
    "gemini-pro",
]

print("="*70)
print("  æµ‹è¯•ä¸åŒæ¨¡å‹")
print("="*70)
print()

for model in models_to_try:
    print(f"ğŸ“¡ æµ‹è¯•æ¨¡å‹: {model}")
    
    try:
        llm_client = create_llm_client(
            provider='openai',
            api_key=API_KEY,
            model=model,
            api_base=API_BASE
        )
        
        response = llm_client.generate(
            prompt="ä½ å¥½ï¼Œè¯·å›å¤'æ”¶åˆ°'",
            temperature=0.7,
            max_tokens=50
        )
        
        if response and "LLMæœåŠ¡æš‚æ—¶ä¸å¯ç”¨" not in response:
            print(f"  âœ… {model} å¯ç”¨ï¼")
            print(f"  å“åº”: {response}")
            print()
            print(f"ğŸ¯ æ‰¾åˆ°å¯ç”¨æ¨¡å‹: {model}")
            break
        else:
            print(f"  âŒ {model} ä¸å¯ç”¨")
    
    except Exception as e:
        error_msg = str(e)
        print(f"  âŒ {model} é”™è¯¯: {error_msg[:100]}")
    
    print()

"""
ç¬¬å››ç»´åº¦ï¼šè¯­ä¹‰-å› æœæµï¼ˆWè½´ï¼‰

æ ¸å¿ƒå®šä¹‰ï¼š
- Wè½´ = Semantic-Causal Flow
- ä¸æ˜¯ç‹¬ç«‹è½´ï¼Œè€Œæ˜¯è´¯ç©¿ä¸‰ç»´ç©ºé—´çš„åŠ¨æ€å…³è”ç»´åº¦
- ä½œç”¨ï¼šä»"æ—¶ç©ºæœ€ä¼˜"å‡çº§ä¸º"ä½“éªŒæœ€ä¼˜"

æ•°å­¦æ¨¡å‹ï¼š
Î¦_4D(x,y,z,w) = Î¦_3D(x,y,z) + F_wc
F_wc = Î´Â·S_sem + ÎµÂ·C_causal

ç‰©ç†ç±»æ¯”ï¼š
- å»¶ç»­çˆ±å› æ–¯å¦å››ç»´æ—¶ç©ºï¼ˆ3Dç©ºé—´ + 1Dæ—¶é—´ï¼‰
- Wè½´æ˜¯å†³ç­–æ—¶ç©ºçš„æ›²ç‡

Author: GAODE Team
Date: 2024-12
"""

from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass
from datetime import datetime
from enum import Enum
import numpy as np

from .models import Location, UserProfile, State, POIType


class SemanticType(Enum):
    """è¯­ä¹‰ç±»å‹"""
    STATIC_VIEWING = "static_viewing"      # é™æ€è§‚èµï¼ˆå›­æ—ã€åšç‰©é¦†ï¼‰
    DYNAMIC_ACTIVITY = "dynamic_activity"  # åŠ¨æ€æ´»åŠ¨ï¼ˆæ¸¸ä¹åœºã€è¿åŠ¨ï¼‰
    RELAXATION = "relaxation"              # ä¼‘é—²æ”¾æ¾ï¼ˆèŒ¶é¦†ã€æ¸©æ³‰ï¼‰
    DINING = "dining"                      # é¤é¥®ç¾é£Ÿ
    SHOPPING = "shopping"                  # è´­ç‰©
    CULTURAL = "cultural"                  # æ–‡åŒ–ä½“éªŒ
    NATURAL = "natural"                    # è‡ªç„¶é£å…‰


class IntensityLevel(Enum):
    """å¼ºåº¦ç­‰çº§"""
    REST = 1          # ä¼‘æ¯ï¼ˆ0-20%ä½“åŠ›æ¶ˆè€—ï¼‰
    LIGHT = 2         # è½»åº¦ï¼ˆ20-40%ï¼‰
    MODERATE = 3      # ä¸­åº¦ï¼ˆ40-60%ï¼‰
    INTENSE = 4       # é«˜å¼ºåº¦ï¼ˆ60-80%ï¼‰
    EXTREME = 5       # æé™ï¼ˆ80-100%ï¼‰


@dataclass
class UserStateVector:
    """ç”¨æˆ·çŠ¶æ€å‘é‡"""
    physical_energy: float  # ä½“åŠ› 0-1
    mental_energy: float    # ç²¾åŠ› 0-1
    mood: float            # å¿ƒæƒ… 0-1
    satiety: float         # é¥±è…¹æ„Ÿ 0-1
    time_pressure: float   # æ—¶é—´å‹åŠ› 0-1
    
    def to_vector(self) -> np.ndarray:
        """è½¬æ¢ä¸ºå‘é‡"""
        return np.array([
            self.physical_energy,
            self.mental_energy,
            self.mood,
            self.satiety,
            self.time_pressure
        ])


@dataclass
class SemanticVector:
    """è¯­ä¹‰å‘é‡"""
    semantic_type: SemanticType
    intensity_level: IntensityLevel
    duration: float  # æŒç»­æ—¶é—´ï¼ˆå°æ—¶ï¼‰
    
    # è¯­ä¹‰å±æ€§
    is_indoor: bool
    is_static: bool
    cultural_depth: float  # æ–‡åŒ–æ·±åº¦ 0-1
    physical_demand: float  # ä½“åŠ›éœ€æ±‚ 0-1
    
    def to_embedding(self) -> np.ndarray:
        """è½¬æ¢ä¸ºåµŒå…¥å‘é‡"""
        # 8ç»´è¯­ä¹‰åµŒå…¥
        return np.array([
            self.intensity_level.value / 5.0,  # å¼ºåº¦å½’ä¸€åŒ–
            self.duration / 4.0,  # å‡è®¾æœ€é•¿4å°æ—¶
            1.0 if self.is_indoor else 0.0,
            1.0 if self.is_static else 0.0,
            self.cultural_depth,
            self.physical_demand,
            self._type_to_numeric(),
            0.0  # é¢„ç•™ç»´åº¦
        ])
    
    def _type_to_numeric(self) -> float:
        """ç±»å‹è½¬æ•°å€¼"""
        type_map = {
            SemanticType.STATIC_VIEWING: 0.1,
            SemanticType.DYNAMIC_ACTIVITY: 0.9,
            SemanticType.RELAXATION: 0.2,
            SemanticType.DINING: 0.5,
            SemanticType.SHOPPING: 0.6,
            SemanticType.CULTURAL: 0.3,
            SemanticType.NATURAL: 0.7
        }
        return type_map.get(self.semantic_type, 0.5)


class SemanticFlowAnalyzer:
    """
    è¯­ä¹‰æµåˆ†æå™¨
    
    åŠŸèƒ½ï¼šè®¡ç®—è¡Œç¨‹çš„ä½“éªŒè¿è´¯æ€§
    - å†…å®¹è¯­ä¹‰è¿è´¯æ€§
    - å¼ºåº¦è¯­ä¹‰äº’è¡¥æ€§
    - ç”¨æˆ·çŠ¶æ€é€‚é…æ€§
    """
    
    def __init__(self):
        # è¯­ä¹‰ç›¸ä¼¼åº¦çŸ©é˜µï¼ˆåŸºäºPOIç±»å‹ï¼‰
        self.semantic_similarity_matrix = self._init_similarity_matrix()
        
        # å¼ºåº¦è½¬ç§»çŸ©é˜µï¼ˆå‰åå¼ºåº¦ç»„åˆçš„åˆç†æ€§ï¼‰
        self.intensity_transition_matrix = self._init_intensity_matrix()
    
    def compute_semantic_score(self,
                              current_poi: Location,
                              next_poi: Location,
                              user_state: UserStateVector,
                              history: List[Location]) -> Tuple[float, str]:
        """
        è®¡ç®—è¯­ä¹‰æµå¾—åˆ†
        
        Returns:
            (S_sem âˆˆ [-1, 1], explanation)
            
        å¾—åˆ†å«ä¹‰ï¼š
        - +1.0: å®Œç¾è¿è´¯
        - +0.5~1.0: è‰¯å¥½è¿è´¯
        - 0: ä¸­æ€§ï¼ˆæ— å…³è”ï¼‰
        - -0.5~0: è½»å¾®å†²çª
        - -1.0: ä¸¥é‡å†²çª
        """
        # æå–è¯­ä¹‰å‘é‡
        current_semantic = self._extract_semantic(current_poi)
        next_semantic = self._extract_semantic(next_poi)
        
        # 1. å†…å®¹è¯­ä¹‰è¿è´¯æ€§ï¼ˆ40%æƒé‡ï¼‰
        content_score = self._compute_content_coherence(
            current_semantic, next_semantic
        )
        
        # 2. å¼ºåº¦è¯­ä¹‰äº’è¡¥æ€§ï¼ˆ30%æƒé‡ï¼‰
        intensity_score = self._compute_intensity_complementarity(
            current_semantic, next_semantic, user_state
        )
        
        # 3. ç”¨æˆ·çŠ¶æ€é€‚é…æ€§ï¼ˆ30%æƒé‡ï¼‰
        state_score = self._compute_state_fitness(
            next_semantic, user_state
        )
        
        # åŠ æƒç»¼åˆ
        S_sem = (
            0.4 * content_score +
            0.3 * intensity_score +
            0.3 * state_score
        )
        
        # ç”Ÿæˆè§£é‡Š
        explanation = self._generate_semantic_explanation(
            content_score, intensity_score, state_score,
            current_poi, next_poi
        )
        
        return S_sem, explanation
    
    def _extract_semantic(self, poi: Location) -> SemanticVector:
        """ä»POIæå–è¯­ä¹‰å‘é‡"""
        # æ ¹æ®POIç±»å‹æ˜ å°„è¯­ä¹‰ç±»å‹
        type_mapping = {
            POIType.ATTRACTION: SemanticType.STATIC_VIEWING,
            POIType.RESTAURANT: SemanticType.DINING,
            POIType.SHOPPING: SemanticType.SHOPPING,
            POIType.HOTEL: SemanticType.RELAXATION,
            POIType.ENTERTAINMENT: SemanticType.DYNAMIC_ACTIVITY
        }
        
        semantic_type = type_mapping.get(poi.type, SemanticType.CULTURAL)
        
        # æ¨æ–­å¼ºåº¦ç­‰çº§
        if semantic_type == SemanticType.STATIC_VIEWING:
            intensity = IntensityLevel.LIGHT
        elif semantic_type == SemanticType.DYNAMIC_ACTIVITY:
            intensity = IntensityLevel.INTENSE
        elif semantic_type == SemanticType.RELAXATION:
            intensity = IntensityLevel.REST
        else:
            intensity = IntensityLevel.MODERATE
        
        return SemanticVector(
            semantic_type=semantic_type,
            intensity_level=intensity,
            duration=getattr(poi, 'average_visit_time', 2.0) or 2.0,
            is_indoor=semantic_type in [SemanticType.SHOPPING, SemanticType.DINING],
            is_static=semantic_type == SemanticType.STATIC_VIEWING,
            cultural_depth=0.8 if semantic_type == SemanticType.CULTURAL else 0.3,
            physical_demand=intensity.value / 5.0
        )
    
    def _compute_content_coherence(self,
                                   current: SemanticVector,
                                   next: SemanticVector) -> float:
        """è®¡ç®—å†…å®¹è¿è´¯æ€§"""
        # ä½¿ç”¨ç›¸ä¼¼åº¦çŸ©é˜µ
        similarity = self.semantic_similarity_matrix.get(
            (current.semantic_type, next.semantic_type),
            0.0
        )
        
        # æ£€æŸ¥å†²çªæ¨¡å¼
        # 1. è¿ç»­é™æ€è§‚èµï¼ˆç–²åŠ³ï¼‰
        if current.is_static and next.is_static:
            if current.duration + next.duration > 3:  # è¿ç»­è¶…è¿‡3å°æ—¶
                similarity -= 0.4
        
        # 2. å®¤å†…/å®¤å¤–äº¤æ›¿ï¼ˆä½“éªŒä¸°å¯Œï¼‰
        if current.is_indoor != next.is_indoor:
            similarity += 0.2
        
        return np.clip(similarity, -1.0, 1.0)
    
    def _compute_intensity_complementarity(self,
                                          current: SemanticVector,
                                          next: SemanticVector,
                                          user_state: UserStateVector) -> float:
        """è®¡ç®—å¼ºåº¦äº’è¡¥æ€§"""
        current_intensity = current.intensity_level.value
        next_intensity = next.intensity_level.value
        
        # å¼ºåº¦å·®
        intensity_diff = abs(current_intensity - next_intensity)
        
        # æ ¹æ®ç”¨æˆ·ä½“åŠ›è¯„ä¼°åˆç†æ€§
        if user_state.physical_energy > 0.7:
            # ä½“åŠ›å……æ²›ï¼šæ¥å—é«˜å¼ºåº¦
            if next_intensity >= 4:
                return 0.8
            else:
                return 0.5
        elif user_state.physical_energy > 0.4:
            # ä½“åŠ›ä¸­ç­‰ï¼šé€‚åˆä¸­åº¦
            if 2 <= next_intensity <= 3:
                return 0.9
            elif next_intensity >= 4:
                return -0.3  # ä¸é€‚åˆé«˜å¼ºåº¦
            else:
                return 0.6
        else:
            # ä½“åŠ›ä½ï¼šéœ€è¦ä¼‘æ¯
            if next_intensity <= 2:
                return 1.0  # å¼ºçƒˆæ¨èä¼‘æ¯
            else:
                return -0.6  # ä¸é€‚åˆç»§ç»­é«˜å¼ºåº¦
        
        # æ£€æŸ¥å¼ºåº¦å åŠ ç–²åŠ³
        if current_intensity >= 4 and next_intensity >= 4:
            return -0.7  # è¿ç»­é«˜å¼ºåº¦ï¼Œä¸¥é‡å†²çª
        
        return 0.0
    
    def _compute_state_fitness(self,
                              next: SemanticVector,
                              user_state: UserStateVector) -> float:
        """è®¡ç®—ç”¨æˆ·çŠ¶æ€é€‚é…æ€§"""
        score = 0.0
        
        # 1. ä½“åŠ›é€‚é…
        physical_fitness = 1.0 - abs(next.physical_demand - user_state.physical_energy)
        score += physical_fitness * 0.4
        
        # 2. ç²¾åŠ›é€‚é…
        if next.is_static and user_state.mental_energy < 0.4:
            score -= 0.3  # ç²¾åŠ›ä¸è¶³æ—¶ä¸é€‚åˆé™æ€è§‚èµ
        
        # 3. é¥±è…¹æ„Ÿé€‚é…
        if next.semantic_type == SemanticType.DINING:
            if user_state.satiety < 0.3:
                score += 0.8  # é¥¿äº†ï¼Œæ¨èé¤é¥®
            elif user_state.satiety > 0.7:
                score -= 0.5  # å¤ªé¥±ï¼Œä¸æ¨è
        
        # 4. å¿ƒæƒ…é€‚é…
        if user_state.mood < 0.4:
            if next.semantic_type in [SemanticType.RELAXATION, SemanticType.NATURAL]:
                score += 0.5  # å¿ƒæƒ…ä¸å¥½ï¼Œæ¨èæ”¾æ¾
        
        return np.clip(score, -1.0, 1.0)
    
    def _generate_semantic_explanation(self,
                                      content_score: float,
                                      intensity_score: float,
                                      state_score: float,
                                      current_poi: Location,
                                      next_poi: Location) -> str:
        """ç”Ÿæˆè¯­ä¹‰è§£é‡Š"""
        explanations = []
        
        if content_score > 0.6:
            explanations.append(f"ä¸{current_poi.name}ä½“éªŒè¿è´¯")
        elif content_score < -0.3:
            explanations.append(f"ä¸{current_poi.name}ä½“éªŒå†²çª")
        
        if intensity_score > 0.7:
            explanations.append("å¼ºåº¦æ­é…åˆç†")
        elif intensity_score < -0.3:
            explanations.append("å¼ºåº¦è¿‡é«˜ï¼Œå»ºè®®ä¼‘æ¯")
        
        if state_score > 0.6:
            explanations.append("ç¬¦åˆæ‚¨å½“å‰çŠ¶æ€")
        elif state_score < -0.3:
            explanations.append("å¯èƒ½ä¸é€‚åˆå½“å‰çŠ¶æ€")
        
        return "ï¼›".join(explanations) if explanations else "ä½“éªŒä¸­æ€§"
    
    def _init_similarity_matrix(self) -> Dict[Tuple[SemanticType, SemanticType], float]:
        """åˆå§‹åŒ–è¯­ä¹‰ç›¸ä¼¼åº¦çŸ©é˜µ"""
        # ç®€åŒ–ç‰ˆï¼šåªå®šä¹‰å…³é”®ç»„åˆ
        return {
            # æ–‡åŒ–ç±»è¿è´¯
            (SemanticType.CULTURAL, SemanticType.STATIC_VIEWING): 0.8,
            (SemanticType.STATIC_VIEWING, SemanticType.CULTURAL): 0.8,
            
            # åŠ¨æ€æ´»åŠ¨åæ¥ä¼‘é—²
            (SemanticType.DYNAMIC_ACTIVITY, SemanticType.RELAXATION): 0.9,
            
            # é¤é¥®åæ¥ä¼‘é—²
            (SemanticType.DINING, SemanticType.RELAXATION): 0.7,
            
            # å†²çªç»„åˆ
            (SemanticType.STATIC_VIEWING, SemanticType.STATIC_VIEWING): -0.2,  # è¿ç»­é™æ€
            (SemanticType.DYNAMIC_ACTIVITY, SemanticType.DYNAMIC_ACTIVITY): -0.4,  # è¿ç»­é«˜å¼ºåº¦
        }
    
    def _init_intensity_matrix(self) -> np.ndarray:
        """åˆå§‹åŒ–å¼ºåº¦è½¬ç§»çŸ©é˜µ"""
        # 5x5çŸ©é˜µï¼ˆ5ä¸ªå¼ºåº¦ç­‰çº§ï¼‰
        # è¡Œ=å½“å‰å¼ºåº¦ï¼Œåˆ—=ä¸‹ä¸€å¼ºåº¦ï¼Œå€¼=åˆç†æ€§å¾—åˆ†
        return np.array([
            # REST  LIGHT  MOD   INTENSE EXTREME
            [0.5,  0.9,   0.8,  0.6,    0.3],   # ä»REST
            [0.7,  0.7,   0.9,  0.7,    0.4],   # ä»LIGHT
            [0.8,  0.8,   0.7,  0.6,    0.3],   # ä»MODERATE
            [0.9,  0.8,   0.6,  0.3,    0.1],   # ä»INTENSE
            [1.0,  0.9,   0.7,  0.2,   -0.3]    # ä»EXTREME
        ])


class CausalFlowAnalyzer:
    """
    å› æœæµåˆ†æå™¨
    
    åŠŸèƒ½ï¼šè®¡ç®—å†³ç­–çš„é€»è¾‘å…³è”æ€§
    - äº‹ä»¶å› æœï¼ˆæ™¯ç‚¹é—­å›­â†’æ¨èåŒç±»å‹å¤‡é€‰ï¼‰
    - å†³ç­–å› æœï¼ˆç”¨æˆ·åå¥½â†’æ¨èç­–ç•¥ï¼‰
    - ç¯å¢ƒå› æœï¼ˆå¤©æ°”â†’åœºæ‰€ç±»å‹ï¼‰
    """
    
    def __init__(self, spatial_intelligence=None, llm_client=None, enable_concurrent=True):
        """
        Args:
            spatial_intelligence: å¤§æ¨¡å‹ï¼ˆä¸Šå¸è§†è§’ï¼‰ï¼Œç”¨äºå› æœæ¨ç†ï¼ˆå…¼å®¹æ—§ç‰ˆï¼‰
            llm_client: LLMå®¢æˆ·ç«¯ï¼ˆæ–°ç‰ˆï¼Œæ¨èä½¿ç”¨ï¼‰
            enable_concurrent: æ˜¯å¦å¯ç”¨å¹¶å‘æ¨ç†
        """
        self.spatial_intelligence = spatial_intelligence
        self.llm_client = llm_client  # ğŸ”¥ æ–°å¢ï¼šLLMå®¢æˆ·ç«¯
        self.enable_concurrent = enable_concurrent  # ğŸ”¥ æ–°å¢ï¼šå¹¶å‘å¼€å…³
        
        # å› æœè§„åˆ™åº“
        self.causal_rules = self._init_causal_rules()
    
    def compute_causal_score(self,
                            current_poi: Location,
                            next_poi: Location,
                            context: Dict,
                            state: State) -> Tuple[float, str]:
        """
        è®¡ç®—å› æœæµå¾—åˆ†
        
        Args:
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå¤©æ°”ã€äº‹ä»¶ã€ç”¨æˆ·åå¥½ç­‰ï¼‰
            
        Returns:
            (C_causal âˆˆ [0, 1], explanation)
            
        å¾—åˆ†å«ä¹‰ï¼š
        - 1.0: å› æœé“¾å®Œå…¨è‡ªæ´½
        - 0.7~1.0: å› æœå…³è”å¼º
        - 0.4~0.7: å› æœå…³è”ä¸­ç­‰
        - 0~0.4: å› æœå…³è”å¼±
        """
        # æå–å› æœé“¾
        causal_chain = self._extract_causal_chain(
            current_poi, next_poi, context, state
        )
        
        # å¦‚æœæœ‰å¤§æ¨¡å‹ï¼Œä½¿ç”¨"ä¸Šå¸è§†è§’"æ¨ç†
        if self.spatial_intelligence:
            llm_score = self._llm_causal_reasoning(
                current_poi, next_poi, context, causal_chain
            )
        else:
            llm_score = 0.5  # é»˜è®¤ä¸­æ€§
        
        # è§„åˆ™åŸºç¡€åˆ†
        rule_score = self._rule_based_causal_score(causal_chain)
        
        # ç»¼åˆå¾—åˆ†ï¼ˆå¤§æ¨¡å‹50%+è§„åˆ™50%ï¼‰
        C_causal = 0.5 * llm_score + 0.5 * rule_score
        
        # ç”Ÿæˆè§£é‡Š
        explanation = self._generate_causal_explanation(causal_chain)
        
        return C_causal, explanation
    
    def _extract_causal_chain(self,
                             current_poi: Location,
                             next_poi: Location,
                             context: Dict,
                             state: State) -> List[Dict]:
        """æå–å› æœé“¾"""
        chain = []
        
        # 1. äº‹ä»¶å› æœ
        if context.get('event_type'):
            chain.append({
                'type': 'event_causal',
                'event': context['event_type'],
                'from': current_poi.name,
                'to': next_poi.name,
                'reason': self._explain_event_causal(context, current_poi, next_poi)
            })
        
        # 2. å†³ç­–å› æœ
        if hasattr(state, 'user_preferences'):
            chain.append({
                'type': 'decision_causal',
                'preference': state.user_preferences,
                'to': next_poi.name,
                'reason': self._explain_decision_causal(state, next_poi)
            })
        
        # 3. ç¯å¢ƒå› æœ
        if context.get('weather'):
            chain.append({
                'type': 'environment_causal',
                'weather': context['weather'],
                'to': next_poi.name,
                'reason': self._explain_environment_causal(context, next_poi)
            })
        
        return chain
    
    def _llm_causal_reasoning(self,
                             current_poi: Location,
                             next_poi: Location,
                             context: Dict,
                             causal_chain: List[Dict]) -> float:
        """
        å¤§æ¨¡å‹å› æœæ¨ç†ï¼ˆä¸Šå¸è§†è§’ï¼‰
        
        åˆ©ç”¨SpatialIntelligenceCoreçš„æ¨ç†èƒ½åŠ›
        """
        try:
            # æ„é€ æ¨ç†é—®é¢˜
            question = f"ä»{current_poi.name}åˆ°{next_poi.name}ï¼Œè€ƒè™‘{context}ï¼Œå› æœå…³è”åº¦å¦‚ä½•ï¼Ÿ"
            
            # è°ƒç”¨å¤§æ¨¡å‹æ¨ç†
            # æ³¨ï¼šSpatialIntelligenceCoreå¯èƒ½éœ€è¦æ‰©å±•å› æœæ¨ç†æ¥å£
            if hasattr(self.spatial_intelligence, 'reason_causality'):
                score = self.spatial_intelligence.reason_causality(
                    current_poi, next_poi, context
                )
                return float(score)
            else:
                # é™çº§ï¼šä½¿ç”¨POIç›¸ä¼¼åº¦
                if hasattr(self.spatial_intelligence, 'poi_graph'):
                    similarity = self.spatial_intelligence.compute_poi_similarity(
                        current_poi, next_poi
                    )
                    return float(similarity)
        except:
            pass
        
        return 0.6  # é»˜è®¤ä¸­ç­‰å…³è”
    
    def _rule_based_causal_score(self, causal_chain: List[Dict]) -> float:
        """åŸºäºè§„åˆ™çš„å› æœå¾—åˆ†"""
        if not causal_chain:
            return 0.5
        
        scores = []
        for link in causal_chain:
            rule_score = self.causal_rules.get(link['type'], 0.5)
            scores.append(rule_score)
        
        return np.mean(scores)
    
    def _explain_event_causal(self, context: Dict, current: Location, next: Location) -> str:
        """è§£é‡Šäº‹ä»¶å› æœ"""
        event = context.get('event_type', 'unknown')
        if event == 'closure':
            if current.type == next.type:
                return f"{current.name}é—­å›­ï¼Œæ¨èåŒç±»å‹{next.name}"
            else:
                return f"{current.name}ä¸å¯ç”¨ï¼Œæ¨èå¤‡é€‰{next.name}"
        return "äº‹ä»¶å“åº”"
    
    def _explain_decision_causal(self, state: State, next: Location) -> str:
        """è§£é‡Šå†³ç­–å› æœ"""
        return f"åŸºäºæ‚¨çš„åå¥½æ¨è{next.name}"
    
    def _explain_environment_causal(self, context: Dict, next: Location) -> str:
        """è§£é‡Šç¯å¢ƒå› æœ"""
        weather = context.get('weather', '')
        if 'rain' in weather.lower() or 'é›¨' in weather:
            if next.type in [POIType.SHOPPING, POIType.RESTAURANT]:
                return f"é›¨å¤©æ¨èå®¤å†…åœºæ‰€{next.name}"
        return f"å¤©æ°”é€‚åˆ{next.name}"
    
    def _generate_causal_explanation(self, causal_chain: List[Dict]) -> str:
        """ç”Ÿæˆå› æœè§£é‡Š"""
        if not causal_chain:
            return "å› æœå…³è”ä¸­ç­‰"
        
        explanations = [link['reason'] for link in causal_chain]
        return "ï¼›".join(explanations)
    
    def _init_causal_rules(self) -> Dict[str, float]:
        """åˆå§‹åŒ–å› æœè§„åˆ™åº“"""
        return {
            'event_causal': 0.9,         # äº‹ä»¶å› æœæƒé‡é«˜
            'decision_causal': 0.7,      # å†³ç­–å› æœä¸­ç­‰
            'environment_causal': 0.8    # ç¯å¢ƒå› æœè¾ƒé«˜
        }
    
    def batch_compute_causal_flow(self, tasks: List[Dict]) -> List[Dict]:
        """
        æ‰¹é‡è®¡ç®—å› æœæµï¼ˆğŸ”¥ ä¿®å¤ï¼šè¿”å›ç»“æ„åŒ–å¼ åŠ›ï¼‰
        
        Args:
            tasks: ä»»åŠ¡åˆ—è¡¨ï¼Œæ¯ä¸ªä»»åŠ¡åŒ…å«ï¼š
                - current: å½“å‰POI
                - next: ä¸‹ä¸€ä¸ªPOI
                - context: ä¸Šä¸‹æ–‡ï¼ˆå¤©æ°”ã€æ—¶é—´ç­‰ï¼‰
                
        Returns:
            å¼ åŠ›ä¿¡æ¯åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å«ï¼š
            {
                'c_causal': ç»¼åˆåˆ†æ•° [0, 1]ï¼Œ
                'tensions': {
                    'novelty': æ–°é²œæ„Ÿå¼ åŠ› [-1, 1]ï¼Œ
                    'continuity': è¿ç»­æ€§å¼ åŠ› [-1, 1]ï¼Œ
                    'energy': ä½“åŠ›å¼ åŠ› [-1, 1]ï¼Œ
                    'conflict': å†²çªåº¦ [0, 1]
                }
            }
        """
        if not tasks:
            return []
        
        # æ–¹æ¡ˆ1ï¼šä½¿ç”¨LLMå®¢æˆ·ç«¯æ‰¹é‡æ¨ç†ï¼ˆæ¨èï¼‰
        if self.llm_client:
            return self._batch_llm_reason_with_tensions(tasks)
        
        # æ–¹æ¡ˆ2ï¼šä½¿ç”¨æ—§ç‰ˆspatial_intelligenceï¼ˆå…¼å®¹ï¼‰
        elif self.spatial_intelligence:
            return self._batch_spatial_reason_with_tensions(tasks)
        
        # æ–¹æ¡ˆ3ï¼šçº¯è§„åˆ™æ¨ç†ï¼ˆé™çº§ï¼‰
        else:
            return self._batch_rule_reason_with_tensions(tasks)
    
    def _batch_llm_reason(self, tasks: List[Dict]) -> List[float]:
        """ä½¿ç”¨LLMå®¢æˆ·ç«¯æ‰¹é‡æ¨ç†"""
        # æ„å»ºprompts
        prompts = []
        for task in tasks:
            current = task['current']
            next_poi = task['next']
            context = task.get('context', {})
            
            # æå–å…³é”®ä¿¡æ¯
            weather = context.get('weather', 'sunny')
            time_hour = context.get('time_of_day', 10)
            visited_regions = context.get('visited_regions', {})
            
            # è®¡ç®—åŒºåŸŸè®¿é—®æ¬¡æ•°
            region = self._get_region(next_poi)
            visit_count = visited_regions.get(region, 0)
            
            # æ„å»ºprompt
            prompt = f"""è¯„ä¼°æ—…è¡Œå†³ç­–åˆç†æ€§ï¼ˆ0-1åˆ†ï¼‰ï¼š

å½“å‰ï¼š{current.name}
å€™é€‰ï¼š{next_poi.name}ï¼ˆ{region}åŒºåŸŸï¼‰
æ—¶é—´ï¼š{time_hour}ç‚¹ | å¤©æ°”ï¼š{weather}
è¯¥åŒºåŸŸå·²è®¿é—®ï¼š{visit_count}æ¬¡

è¯„ä¼°è¦ç‚¹ï¼š
1. åŒºåŸŸé‡å¤ï¼šé¦–æ¬¡+0.3ï¼Œç¬¬2æ¬¡-0.25ï¼Œç¬¬3æ¬¡-0.4
2. æ—¶é—´åˆç†ï¼šä¸­åˆé¤å…+0.4ï¼Œå…¶ä»–æ—¶æ®µé¤å…-0.2
3. å¤©æ°”é€‚é…ï¼šé›¨å¤©å®¤å†…+0.2ï¼Œé›¨å¤©æˆ·å¤–-0.3
4. æ™¯ç‚¹çŸ¥ååº¦ï¼šçŸ¥åæ™¯ç‚¹+0.15
5. ç±»å‹è¿ç»­ï¼šé‡å¤ç±»å‹-0.15

åªè¿”å›ä¸€ä¸ª0-1ä¹‹é—´çš„æ•°å­—ï¼ˆå¦‚0.85ï¼‰ï¼Œä¸è¦è§£é‡Šã€‚"""
            
            prompts.append(prompt)
        
        # å¹¶å‘è°ƒç”¨LLM
        results = self.llm_client.batch_reason(
            prompts, 
            temperature=0.5, 
            max_tokens=10,
            max_workers=10 if self.enable_concurrent else 1
        )
        
        # å¤„ç†ç»“æœï¼ˆNone â†’ è§„åˆ™æ¨ç†ï¼‰
        final_results = []
        for i, score in enumerate(results):
            if score is not None:
                final_results.append(float(score))
            else:
                # é™çº§åˆ°è§„åˆ™
                final_results.append(self._rule_causal_score_simple(tasks[i]))
        
        return final_results
    
    def _batch_spatial_reason(self, tasks: List[Dict]) -> List[float]:
        """ä½¿ç”¨æ—§ç‰ˆspatial_intelligenceæ‰¹é‡æ¨ç†ï¼ˆå…¼å®¹ï¼‰"""
        results = []
        for task in tasks:
            try:
                if hasattr(self.spatial_intelligence, 'reason_causality'):
                    score = self.spatial_intelligence.reason_causality(
                        task['current'], task['next'], task.get('context', {})
                    )
                    results.append(float(score))
                else:
                    results.append(0.6)  # é»˜è®¤
            except:
                results.append(0.6)
        return results
    
    def _batch_rule_reason(self, tasks: List[Dict]) -> List[float]:
        """çº¯è§„åˆ™æ‰¹é‡æ¨ç†ï¼ˆé™çº§ï¼Œå‘åå…¼å®¹ï¼‰"""
        return [self._rule_causal_score_simple(task) for task in tasks]
    
    def _batch_rule_reason_with_tensions(self, tasks: List[Dict]) -> List[Dict]:
        """
        çº¯è§„åˆ™æ‰¹é‡æ¨ç†ï¼ˆè¿”å›å¼ åŠ›ï¼‰ğŸ”¥
        """
        results = []
        for task in tasks:
            tensions = self._compute_tensions(task)
            c_causal = self._rule_causal_score_simple(task)
            
            results.append({
                'c_causal': c_causal,
                'tensions': tensions
            })
        
        return results
    
    def _batch_llm_reason_with_tensions(self, tasks: List[Dict]) -> List[Dict]:
        """
        LLMæ‰¹é‡æ¨ç†ï¼ˆè¿”å›å¼ åŠ›ï¼‰ğŸ”¥
        
        å…ˆç”¨è§„åˆ™è®¡ç®—å¼ åŠ›ï¼Œå†ç”¨LLMå¾®è°ƒc_causal
        """
        results = []
        
        # å…ˆç”¨è§„åˆ™è®¡ç®—å¼ åŠ›
        for task in tasks:
            tensions = self._compute_tensions(task)
            results.append({
                'c_causal': 0.5,  # ä¸´æ—¶å€¼
                'tensions': tensions
            })
        
        # ç„¶åç”¨LLMæ‰¹é‡è®¡ç®—c_causalï¼ˆä»ç„¶å¹¶å‘ï¼‰
        c_causals = self._batch_llm_reason(tasks)
        
        # åˆå¹¶ç»“æœ
        for i, c_causal in enumerate(c_causals):
            if c_causal is not None:
                results[i]['c_causal'] = c_causal
            else:
                # LLMå¤±è´¥ï¼Œç”¨è§„åˆ™è®¡ç®—
                results[i]['c_causal'] = self._rule_causal_score_simple(tasks[i])
        
        return results
    
    def _batch_spatial_reason_with_tensions(self, tasks: List[Dict]) -> List[Dict]:
        """
        æ—§ç‰ˆspatial_intelligenceæ¨ç†ï¼ˆè¿”å›å¼ åŠ›ï¼‰ğŸ”¥
        """
        results = []
        for task in tasks:
            tensions = self._compute_tensions(task)
            
            try:
                if hasattr(self.spatial_intelligence, 'reason_causality'):
                    c_causal = float(self.spatial_intelligence.reason_causality(
                        task['current'], task['next'], task.get('context', {})
                    ))
                else:
                    c_causal = 0.6
            except:
                c_causal = 0.6
            
            results.append({
                'c_causal': c_causal,
                'tensions': tensions
            })
        
        return results
    
    def _rule_causal_score_simple(self, task: Dict) -> float:
        """
        ç®€åŒ–è§„åˆ™æ¨ç†ï¼ˆç”¨äºé™çº§ï¼‰
        
        ğŸ”¥ ä¿®å¤ï¼šè¿”å›ç»“æ„åŒ–å¼ åŠ›ï¼Œè€Œä¸æ˜¯å•ä¸€æ ‡é‡
        """
        current = task['current']
        next_poi = task['next']
        context = task.get('context', {})
        
        # ğŸ”¥ è®¡ç®—ä¸‰ä¸ªå­å¼ åŠ›
        tensions = self._compute_tensions(task)
        
        # ç»¼åˆæˆæœ€ç»ˆåˆ†æ•°ï¼ˆä½†ä¿ç•™å¼ åŠ›ä¿¡æ¯ï¼‰
        score = 0.5 + tensions['novelty'] * 0.3 + tensions['continuity'] * 0.2 + tensions['energy'] * 0.1
        
        return max(0.1, min(0.95, score))
    
    def _compute_tensions(self, task: Dict) -> Dict[str, float]:
        """
        è®¡ç®—å­å¼ åŠ›ï¼ˆğŸ”¥ æ ¸å¿ƒä¿®å¤ï¼‰
        
        Returns:
            {
                'novelty': æ–°é²œæ„Ÿå¼ åŠ› [-1, 1]ï¼Œæ­£=æ–°å¥‡ï¼Œè´Ÿ=é‡å¤
                'continuity': è¿ç»­æ€§å¼ åŠ› [-1, 1]ï¼Œæ­£=è¿è´¯ï¼Œè´Ÿ=è·³è·ƒ
                'energy': ä½“åŠ›å¼ åŠ› [-1, 1]ï¼Œæ­£=å……æ²›ï¼Œè´Ÿ=ç–²æƒ«
                'conflict': å†²çªåº¦ [0, 1]ï¼Œè¶Šé«˜è¶ŠçŸ›ç›¾
            }
        """
        current = task['current']
        next_poi = task['next']
        context = task.get('context', {})
        
        # 1. æ–°é²œæ„Ÿå¼ åŠ›ï¼ˆnovelty tensionï¼‰
        visited_regions = context.get('visited_regions', {})
        region = self._get_region(next_poi)
        visit_count = visited_regions.get(region, 0)
        
        if visit_count == 0:
            novelty = 0.8  # æ–°åŒºåŸŸï¼Œå¼ºçƒˆå¸å¼•
        elif visit_count == 1:
            novelty = -0.3  # å›è®¿ï¼Œè½»åº¦æ’æ–¥
        else:
            novelty = -0.6  # å¤šæ¬¡å›è®¿ï¼Œå¼ºçƒˆæ’æ–¥
        
        # 2. è¿ç»­æ€§å¼ åŠ›ï¼ˆcontinuity tensionï¼‰
        if current.type == next_poi.type:
            continuity = -0.4  # ç±»å‹é‡å¤ï¼Œä½“éªŒå•è°ƒ
        else:
            continuity = 0.3  # ç±»å‹åˆ‡æ¢ï¼Œä½“éªŒä¸°å¯Œ
        
        # çŸ¥ååº¦å½±å“è¿ç»­æ€§
        famous = ["å¦å¤§", "é¼“æµªå±¿", "ç¯å²›è·¯", "æ›¾ååµ", "ä¸­å±±è·¯",
                  "æ‹™æ”¿å›­", "è™ä¸˜", "å¹³æ±Ÿè·¯", "å§‘è‹"]
        if any(f in next_poi.name for f in famous):
            continuity += 0.2  # çŸ¥åæ™¯ç‚¹ï¼Œé€»è¾‘è¿è´¯
        
        # 3. ä½“åŠ›å¼ åŠ›ï¼ˆenergy tensionï¼‰
        time_hour = context.get('time_of_day', 10)
        
        # æ—¶é—´è¶Šæ™šï¼Œä½“åŠ›è¶Šä½
        if time_hour < 12:
            energy = 0.6  # æ—©ä¸Šç²¾åŠ›å……æ²›
        elif time_hour < 16:
            energy = 0.2  # ä¸‹åˆé€‚ä¸­
        elif time_hour < 18:
            energy = -0.2  # å‚æ™šå¼€å§‹ç–²æƒ«
        else:
            energy = -0.5  # æ™šä¸Šå¾ˆç´¯
        
        # é¤å…è¡¥å……ä½“åŠ›
        if next_poi.type.value == 'restaurant':
            if 11 <= time_hour <= 13 or 17 <= time_hour <= 19:
                energy += 0.4  # é¥­ç‚¹åƒé¥­ï¼Œæ¢å¤ä½“åŠ›
        
        # 4. ğŸ”¥ å†²çªåº¦ï¼ˆconflictï¼‰- æ ¸å¿ƒåˆ›æ–°
        # å½“å¤šä¸ªå¼ åŠ›æ–¹å‘ä¸ä¸€è‡´æ—¶ï¼Œå†²çªåº¦é«˜
        tension_values = [novelty, continuity, energy]
        positive_count = sum(1 for t in tension_values if t > 0)
        negative_count = sum(1 for t in tension_values if t < 0)
        
        if positive_count > 0 and negative_count > 0:
            # æœ‰æ­£æœ‰è´Ÿï¼Œå­˜åœ¨å†²çª
            conflict = min(positive_count, negative_count) / len(tension_values)
        else:
            # æ–¹å‘ä¸€è‡´ï¼Œæ— å†²çª
            conflict = 0.0
        
        return {
            'novelty': novelty,
            'continuity': continuity,
            'energy': energy,
            'conflict': conflict
        }
    
    def _get_region(self, poi: Location) -> str:
        """è·å–POIæ‰€å±åŒºåŸŸ"""
        for k in ["é¼“æµªå±¿", "å¦å¤§", "æ›¾ååµ", "ä¸­å±±è·¯", "ç¯å²›è·¯"]:
            if k in poi.name or k in poi.address:
                return k
        return "å…¶ä»–"


class SemanticCausalFlow:
    """
    è¯­ä¹‰-å› æœæµï¼ˆWè½´ï¼‰
    
    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. è®¡ç®—è¯­ä¹‰æµå¾—åˆ† S_sem
    2. è®¡ç®—å› æœæµå¾—åˆ† C_causal
    3. ç”Ÿæˆå…³è”åœºåŠ› F_wc = Î´Â·S_sem + ÎµÂ·C_causal
    4. å åŠ åˆ°Zè½´ï¼Œå‡çº§ä¸ºå››ç»´åŠ¿èƒ½
    5. æ‰¹é‡å¹¶å‘æ¨ç†ï¼ˆğŸ”¥ æ€§èƒ½ä¼˜åŒ–ï¼‰
    
    ç‰©ç†æ„ä¹‰ï¼š
    - Wè½´æ˜¯å†³ç­–æ—¶ç©ºçš„"æ›²ç‡"
    - è®©ä¸‰ç»´ç©ºé—´çš„åŠ¿èƒ½åˆ†å¸ƒæ›´è´´åˆä½“éªŒé€»è¾‘
    """
    
    def __init__(self,
                 spatial_intelligence=None,
                 llm_client=None,
                 delta: float = 0.1,
                 epsilon: float = 0.1,
                 enable_concurrent: bool = True):
        """
        Args:
            spatial_intelligence: å¤§æ¨¡å‹ï¼ˆä¸Šå¸è§†è§’ï¼Œå…¼å®¹æ—§ç‰ˆï¼‰
            llm_client: LLMå®¢æˆ·ç«¯ï¼ˆæ–°ç‰ˆï¼Œæ¨èä½¿ç”¨ï¼‰ğŸ”¥
            delta: è¯­ä¹‰æƒé‡ï¼ˆé»˜è®¤0.1ï¼Œä¸å–§å®¾å¤ºä¸»ï¼‰
            epsilon: å› æœæƒé‡ï¼ˆé»˜è®¤0.1ï¼‰
            enable_concurrent: å¯ç”¨å¹¶å‘æ¨ç†ï¼ˆé»˜è®¤Trueï¼‰ğŸ”¥
        """
        self.semantic_analyzer = SemanticFlowAnalyzer()
        self.causal_analyzer = CausalFlowAnalyzer(
            spatial_intelligence=spatial_intelligence,
            llm_client=llm_client,  # ğŸ”¥ ä¼ é€’LLMå®¢æˆ·ç«¯
            enable_concurrent=enable_concurrent  # ğŸ”¥ ä¼ é€’å¹¶å‘å¼€å…³
        )
        
        self.delta = delta
        self.epsilon = epsilon
        self.llm_client = llm_client  # ğŸ”¥ ä¿å­˜å¼•ç”¨
        
        print(f"âœ… Wè½´åˆå§‹åŒ–å®Œæˆï¼ˆÎ´={delta}, Îµ={epsilon}ï¼‰")
    
    def compute_w_axis_force(self,
                            current_poi: Location,
                            next_poi: Location,
                            user_state: UserStateVector,
                            context: Dict,
                            state: State,
                            history: List[Location]) -> Tuple[float, Dict]:
        """
        è®¡ç®—Wè½´å…³è”åœºåŠ›
        
        Returns:
            (F_wc, details)
            
        æ•°å­¦æ¨¡å‹ï¼š
        F_wc = Î´Â·S_sem + ÎµÂ·C_causal
        """
        # 1. è®¡ç®—è¯­ä¹‰æµå¾—åˆ†
        S_sem, semantic_explanation = self.semantic_analyzer.compute_semantic_score(
            current_poi, next_poi, user_state, history
        )
        
        # 2. è®¡ç®—å› æœæµå¾—åˆ†
        C_causal, causal_explanation = self.causal_analyzer.compute_causal_score(
            current_poi, next_poi, context, state
        )
        
        # 3. è®¡ç®—å…³è”åœºåŠ›
        F_wc = self.delta * S_sem + self.epsilon * C_causal
        
        # 4. è¿”å›è¯¦æƒ…
        details = {
            'S_sem': S_sem,
            'semantic_explanation': semantic_explanation,
            'C_causal': C_causal,
            'causal_explanation': causal_explanation,
            'F_wc': F_wc,
            'delta': self.delta,
            'epsilon': self.epsilon
        }
        
        return F_wc, details
    
    def upgrade_to_4d_potential(self,
                                phi_3d: float,
                                f_wc: float) -> float:
        """
        å‡çº§åˆ°å››ç»´åŠ¿èƒ½
        
        Î¦_4D = Î¦_3D + F_wc
        """
        return phi_3d + f_wc
    
    def batch_compute_causal_flow(self, tasks: List[Dict]) -> List[float]:
        """
        æ‰¹é‡è®¡ç®—å› æœæµï¼ˆğŸ”¥ æ ¸å¿ƒé›†æˆç‚¹ï¼‰
        
        è¿™æ˜¯è¿æ¥æ¶æ„å’Œè„šæœ¬æµ‹è¯•çš„å…³é”®æ–¹æ³•ï¼
        
        Args:
            tasks: ä»»åŠ¡åˆ—è¡¨ï¼Œæ¯ä¸ªä»»åŠ¡åŒ…å«ï¼š
                - current: å½“å‰POI
                - next: ä¸‹ä¸€ä¸ªPOI  
                - context: ä¸Šä¸‹æ–‡ï¼ˆå¤©æ°”ã€æ—¶é—´ã€visited_regionsç­‰ï¼‰
                
        Returns:
            C_causalå¾—åˆ†åˆ—è¡¨
            
        æ€§èƒ½ï¼š
            - å¹¶å‘LLMï¼š10xæé€Ÿï¼ˆå¦‚xiamen_final.pyï¼‰
            - å…¼å®¹æ—§ä»£ç ï¼ˆæ— LLMæ—¶é™çº§åˆ°è§„åˆ™ï¼‰
            
        Example:
            >>> tasks = [
            ...     {'current': poi1, 'next': poi2, 'context': {...}},
            ...     {'current': poi1, 'next': poi3, 'context': {...}},
            ... ]
            >>> scores = w_axis.batch_compute_causal_flow(tasks)
            >>> # [0.75, 0.35, ...]  # å¹¶å‘è®¡ç®—ï¼Œå¿«é€Ÿè¿”å›
        """
        return self.causal_analyzer.batch_compute_causal_flow(tasks)
